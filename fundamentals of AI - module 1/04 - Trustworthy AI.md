# Trustworthy AI

## Should AI be regulated?

Lots of AI applications need trust in the system: if you don't trust the system, you can't follow the decisions it suggests. **However**, there are dangerous applications, like *autonomous weapons*. AI allows us to create things like *deepfakes*, and that's good (ðŸ˜¯), but that's a problem too! 

The EU created a high-level expert group to divise **trustworthy AI guidelines**. Essentially, there are two main components to it: 

- It should respect **fundamental rights**, applicable regulation and core principles and values, ensuring an *ethical purpose*;
- It should be techincally robust and reliable since, even with good intentions, a lack of technological mastery can cause unintentional harm.

The group has created some requirements from the earliest design phase, like *accountability*, *data governance*...

### Properties that a system should rely on

- **Fairness**: a system should be non-discriminatory. E.g. Amazon had to stop a system that helped in recruiting employees by analyzing CVs and only chose male profiles
- **Transparency**: a system behaviour should be understandable 
- **Verifiable**
- **Explainable**: this can be difficult when working with NN
- **Accountability**: responsibility for the taken decision
- Accuracy, Privacy...

An example of how data could be useful is that in 2015, 58k volunteers made their Facebook likes available and patterns where discovered, for example you could get if a person was single or in a relationship, used drugs, was gay...

**Explainability is an open challenge.** It is not easy to make sub-symbolic systems explainable. The aim is integrating deep learning with symbolic systems.

## Towards a beneficial AI

The problem requires changing the definition of AI, from a field concerned with pure intelligence, to a field concerned with systems that are a provably beneficial to humans.

**Computational sustainability** is a mix of techniques (from AI too) to deal with big societal challenges. We have three pillars in sustainability: economical, social, environmental. 

An example of beneficial AI is *flood management*, i.e. creating a model to predicts floods in a 50-zones residential area, to evacuate citizens to 10 evacuation centers.

You have a *descriptive model*, describing the roads and their users. You have a *predictive model*, simulating the flood, that helps recognizing whether the water will cross a road segment. Then there's a *prescriptive model*, that produces an evacuation plan that tries to get each family to the evacuation zones in their own vehicle. So the thing is: *are we sure that this model works?* Of course not: you have to deal with people, i.e. the infamous **human factor**. Plans should model the human aspects, and **the ethical ones too.**

The high-impact domains Stanford University supports are *transports, domotics, health, education, inclusion of poor classes, safety adn security, job market, entertainment.*



